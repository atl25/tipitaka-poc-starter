# Tipitaka POC Starter ‚Äì One‚ÄëClick ETL + Search

## üóÇ Project Structure

```
tipitaka-poc-starter/
‚îú‚îÄ‚îÄ docker-compose.yml              # your compose (example provided below)
‚îú‚îÄ‚îÄ bootstrap.sh                    # one-touch orchestrator
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ download_folder.py          # downloads 'outputs/' from your Drive (optional)
‚îî‚îÄ‚îÄ etl/
    ‚îú‚îÄ‚îÄ Dockerfile.etl              # container for ETL steps
    ‚îú‚îÄ‚îÄ requirements.txt            # Python deps for ETL image
    ‚îî‚îÄ‚îÄ app/
        ‚îú‚îÄ‚îÄ pipeline.py             # ETL entrypoint (schema + CSV + vectors + sanity search)
        ‚îú‚îÄ‚îÄ insert_vectors_generic.py
        ‚îú‚îÄ‚îÄ weaviate_multitier_setup_and_search_patched.py
        ‚îî‚îÄ‚îÄ search_weaviate_labse_hybridfix.py
```

Your **model/data** artifacts are expected under **`data/outputs/`** (either pre-mounted or downloaded at runtime):

```
data/outputs/
‚îú‚îÄ‚îÄ windows_with_headings.csv
‚îú‚îÄ‚îÄ sentences_with_headings.csv
‚îú‚îÄ‚îÄ windows_2_3.csv
‚îú‚îÄ‚îÄ sentences_from_200.csv
‚îú‚îÄ‚îÄ subchunks_200.csv
‚îú‚îÄ‚îÄ chunks.csv
‚îú‚îÄ‚îÄ windows_ids.txt           ‚îê
‚îú‚îÄ‚îÄ windows_labse.npy         ‚îÇ
‚îú‚îÄ‚îÄ sentences_ids.txt         ‚îÇ  LaBSE vectors + ids (must align)
‚îú‚îÄ‚îÄ sentences_labse.npy       ‚îÇ
‚îú‚îÄ‚îÄ subchunks_ids.txt         ‚îÇ
‚îú‚îÄ‚îÄ subchunks_labse.npy       ‚îÇ
‚îú‚îÄ‚îÄ chunks_ids.txt            ‚îÇ
‚îî‚îÄ‚îÄ chunks_labse.npy          ‚îò
```


## ‚úÖ Prerequisites

Docker (Docker Desktop on Windows/macOS, Docker Engine on Linux)

Windows ·Äô·Äæ·Ä¨ Git Bash (·Äû·Ä≠·ÄØ·Ä∑) PowerShell/CMD ·ÄÄ·Ä≠·ÄØ ·Ä°·Äû·ÄØ·Ä∂·Ä∏·Äï·Äº·ÄØ·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äï·Ä´·Äê·Äö·Ä∫


üöÄ One-Click Setup

Git Bash (recommended):

cd tipitaka-poc-starter
./bootstrap.sh setup


setup ·Äû·Ää·Ä∫ ·Ä°·Äú·Ä≠·ÄØ·Ä°·Äú·Äª·Ä±·Ä¨·ÄÄ·Ä∫ ·Ä°·Ä±·Ä¨·ÄÄ·Ä∫·Äï·Ä´ ·Ä°·ÄÜ·ÄÑ·Ä∑·Ä∫·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·Äú·ÄØ·Äï·Ä∫·ÄÜ·Ä±·Ä¨·ÄÑ·Ä∫·Äï·Ä´·Äê·Äö·Ä∫‚Äî

up ‚Äî Weaviate container run

build ‚Äî ETL image build

load ‚Äî pipeline.py ·ÄÄ·Ä≠·ÄØ ch·∫°y (wait ‚Üí schema ‚Üí CSV ‚Üí vectors ‚Üí sanity search)


üß† What the pipeline does

etl/app/pipeline.py (simple runner):

Wait for Weaviate readiness (/v1/.well-known/ready or OK)

Schema setup ‚Äî 4 collections: Window, Sentence, Subchunk, Chunk

CSV Insert (BM25) ‚Äî class ·Äê·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ CSV ·Äê·ÄÖ·Ä∫·ÄÅ·ÄØ·ÄÅ·Äª·ÄÑ·Ä∫·Ä∏·ÄÖ·ÄÆ ·Äê·ÄÑ·Ä∫

windows_with_headings.csv ·Äô·Äõ·Äæ·Ä≠·Äõ·ÄÑ·Ä∫ windows_2_3.csv fallback

sentences_with_headings.csv ·Äô·Äõ·Äæ·Ä≠·Äõ·ÄÑ·Ä∫ sentences_from_200.csv fallback

Vector Insert (·Äõ·Äæ·Ä≠·Äî·Ä±·Äú·Äª·Äæ·ÄÑ·Ä∫·Äû·Ä¨) ‚Äî insert_vectors_generic.py ·Äî·Ä≤·Ä∑ ids/npy ·Äê·ÄÑ·Ä∫

Sanity Search ‚Äî Window collection ·Äï·Ä±·Ä´·Ä∫ mettƒÅ ·ÄÄ·Ä≠·ÄØ ·ÄÖ·Äô·Ä∫·Ä∏·Äõ·Äæ·Ä¨


# Search Quick Reference

# ---------- Bash / Git Bash / WSL ----------
# Window
python etl/app/search_weaviate_labse_hybridfix.py --url http://localhost:8080 --grpc-port 50051 --collection Window --mode bm25   --query "mettƒÅ" --k 5
python etl/app/search_weaviate_labse_hybridfix.py --url http://localhost:8080 --grpc-port 50051 --collection Window --mode vector --query "mettƒÅ" --k 5
python etl/app/search_weaviate_labse_hybridfix.py --url http://localhost:8080 --grpc-port 50051 --collection Window --mode hybrid --alpha 0.5 --query "mettƒÅ" --k 5

# Sentence
python etl/app/search_weaviate_labse_hybridfix.py --url http://localhost:8080 --grpc-port 50051 --collection Sentence --mode bm25   --query "Admonish (garahati)" --k 5
python etl/app/search_weaviate_labse_hybridfix.py --url http://localhost:8080 --grpc-port 50051 --collection Sentence --mode vector --query "Admonish (garahati)" --k 5
python etl/app/search_weaviate_labse_hybridfix.py --url http://localhost:8080 --grpc-port 50051 --collection Sentence --mode hybrid --alpha 0.5 --query "Admonish (garahati)" --k 5

# Subchunk
python etl/app/search_weaviate_labse_hybridfix.py --url http://localhost:8080 --grpc-port 50051 --collection Subchunk --mode bm25   --query "·Ä°·Äî·Ä≠·ÄÖ·Äπ·ÄÖ" --k 5
python etl/app/search_weaviate_labse_hybridfix.py --url http://localhost:8080 --grpc-port 50051 --collection Subchunk --mode vector --query "·Ä°·Äî·Ä≠·ÄÖ·Äπ·ÄÖ" --k 5
python etl/app/search_weaviate_labse_hybridfix.py --url http://localhost:8080 --grpc-port 50051 --collection Subchunk --mode hybrid --alpha 0.5 --query "·Ä°·Äî·Ä≠·ÄÖ·Äπ·ÄÖ" --k 5

# Chunk
python etl/app/search_weaviate_labse_hybridfix.py --url http://localhost:8080 --grpc-port 50051 --collection Chunk --mode bm25   --query "Four Noble Truths" --k 5
python etl/app/search_weaviate_labse_hybridfix.py --url http://localhost:8080 --grpc-port 50051 --collection Chunk --mode vector --query "Four Noble Truths" --k 5
python etl/app/search_weaviate_labse_hybridfix.py --url http://localhost:8080 --grpc-port 50051 --collection Chunk --mode hybrid --alpha 0.5 --query "Four Noble Truths" --k 5

# ---------- Windows PowerShell ----------
# (PowerShell prefers using python .\script.py)
python .\etl\app\search_weaviate_labse_hybridfix.py --url http://localhost:8080 --grpc-port 50051 --collection Sentence --mode bm25   --query "Admonish (garahati)" --k 5
python .\etl\app\search_weaviate_labse_hybridfix.py --url http://localhost:8080 --grpc-port 50051 --collection Sentence --mode vector --query "Admonish (garahati)" --k 5
python .\etl\app\search_weaviate_labse_hybridfix.py --url http://localhost:8080 --grpc-port 50051 --collection Sentence --mode hybrid --alpha 0.5 --query "Admonish (garahati)" --k 5

# ---------- Windows CMD ----------
cd /d D:\tipitaka-poc-starter
python etl\app\search_weaviate_labse_hybridfix.py --url http://localhost:8080 --grpc-port 50051 --collection Window --mode hybrid --alpha 0.5 --query "mettƒÅ" --k 5

# ---------- Run inside Docker (no local Python needed) ----------
# (Container-to-container URL ·Äû·ÄØ·Ä∂·Ä∏·Äë·Ä¨·Ä∏·Äï·Äº·ÄÆ·Ä∏ --url http://weaviate:8080)
docker compose run --rm etl python etl/app/search_weaviate_labse_hybridfix.py --url http://weaviate:8080 --grpc-port 50051 --collection Window --mode hybrid --alpha 0.5 --query "mettƒÅ" --k 5









## üöÄ Quick Start (recommended)

1) **Clone** this repo and place your `outputs/` under `data/outputs/`  
   ‚Äì *OR* set the envs below so the container downloads it automatically.

2) **Start everything** in one command:
   ```bash
   ./bootstrap.sh setup
   ```
   This will:
   - Bring up Weaviate
   - Build the ETL image
   - Run the ETL pipeline:
     - wait for Weaviate health
     - create/refresh schema
     - insert CSV for BM25
     - insert vectors for Window/Sentence/Subchunk/Chunk (if files exist)
     - run a tiny sanity search

3) **Search**
   - **GraphQL**: open http://localhost:8080/v1/graphql
   - **CLI** (examples below)


## ‚öôÔ∏è Environment (defaults & overrides)

`etl/app/pipeline.py` honors the following environment variables (you can set them in `docker-compose.yml` or your shell):

```
WEAVIATE_URL=http://weaviate:8080
WEAVIATE_GRPC_PORT=50051
DATA_DIR=/workspace/data
OUTPUTS_DIR=/workspace/data/outputs
DOWNLOAD_OUTPUTS=0            # set 1 to force download via data/download_folder.py
GOOGLE_DRIVE_FOLDER_ID=       # your Drive folder ID (optional)
```

### Two ways to provide `outputs/`

- **Bind mount (fastest)**: map your host `./data/outputs` into the ETL container at `/workspace/data/outputs`.
- **Download at runtime**: set `DOWNLOAD_OUTPUTS=1` and `GOOGLE_DRIVE_FOLDER_ID=XXXX` and ensure `data/download_folder.py` supports that ID/flags.


## üê≥ Example `docker-compose.yml` (minimal)

> If you already have one, keep it. Otherwise, here is a minimal reference.

```yaml
version: "3.9"
services:
  weaviate:
    image: semitechnologies/weaviate:1.24.10
    restart: unless-stopped
    ports:
      - "8080:8080"
      - "50051:50051"
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: "true"
      DEFAULT_VECTORIZER_MODULE: "none"        # we use offline LaBSE vectors
      CLUSTER_HOSTNAME: "node1"
    volumes:
      - ./weaviate_data:/var/lib/weaviate

  etl:
    build:
      context: .
      dockerfile: etl/Dockerfile.etl
    environment:
      WEAVIATE_URL: "http://weaviate:8080"
      WEAVIATE_GRPC_PORT: "50051"
      DATA_DIR: "/workspace/data"
      OUTPUTS_DIR: "/workspace/data/outputs"
      DOWNLOAD_OUTPUTS: "0"                    # set "1" to download via data/download_folder.py
      GOOGLE_DRIVE_FOLDER_ID: ""               # set if DOWNLOAD_OUTPUTS=1
    volumes:
      - ./data:/workspace/data                 # bind-mount so ETL sees ./data/outputs
    depends_on:
      - weaviate
```


## üîé How to search

### A) CLI (inside container via compose)

**Vector / BM25 / HYBRID** (HYBRID requires passing a query vector ‚Äî this script does it for you):

```bash
# Window (HYBRID) ‚Äì BM25 + LaBSE vector
docker compose run --rm etl python etl/app/search_weaviate_labse_hybridfix.py \
  --collection Window --mode hybrid --query "four noble truths" --k 5 --alpha 0.5

# Subchunk (vector)
docker compose run --rm etl python etl/app/search_weaviate_labse_hybridfix.py \
  --collection Subchunk --mode vector --query "visuddhimagga" --k 5
```

### B) GraphQL (Browser)

Open **http://localhost:8080/v1/graphql** and paste a query.

**BM25 example**
```graphql
query Bm25Sentence($q: String!) {
  Get {
    Sentence(bm25: { query: $q, properties: ["sentence_text"] }, limit: 5) {
      sentence_id
      sentence_text
      path
      h1 h2 h3 h4
    }
  }
}
```
Variables:
```json
{ "q": "karu·πáƒÅ" }
```

**Vector example**
```graphql
query VecWindow($vec: [Float!]!) {
  Get {
    Window(nearVector: { vector: $vec }, limit: 5) {
      window_id
      text
      path
      h1 h2 h3 h4
    }
  }
}
```

> To produce `$vec`, run in a terminal:
> ```bash
> python -c "from sentence_transformers import SentenceTransformer;import json; \
> m=SentenceTransformer('sentence-transformers/LaBSE'); \
> v=m.encode(['mettƒÅ'],convert_to_numpy=True)[0].astype('float32').tolist(); \
> print(json.dumps(v))"
> ```

**HYBRID example** (vectorizer=none ‚áí you MUST pass a vector)
```graphql
query HybridSub($q: String!, $vec: [Float!]!, $alpha: Float!) {
  Get {
    Subchunk(hybrid: { query: $q, vector: $vec, alpha: $alpha }, limit: 12) {
      subchunk_id
      subchunk_text
      path
    }
  }
}
```
Variables:
```json
{ "q": "patthayamƒÅna", "vec": [/* vector here */], "alpha": 0.5 }
```


## üß∞ Commands (bootstrap)

```bash
./bootstrap.sh setup   # weaviate up ‚Üí build etl ‚Üí run pipeline (one shot)
./bootstrap.sh up      # start weaviate only
./bootstrap.sh load    # run ETL pipeline only
./bootstrap.sh logs    # tail weaviate logs
./bootstrap.sh down    # stop & remove containers
```


## üîÑ Updating data

- Replace files under `data/outputs/` and run:
  ```bash
  ./bootstrap.sh load
  ```
- If you changed schema drastically, consider recreating Weaviate state:
  ```bash
  ./bootstrap.sh down
  rm -rf weaviate_data
  ./bootstrap.sh setup
  ```


## ‚ùó Troubleshooting

- **Hybrid error** `VectorFromInput ... without vectorizer`  
  Your class uses `vectorizer=none`. Hybrid requires both **query text** and **query vector**.  
  Use `search_weaviate_labse_hybridfix.py` (it passes `vector=` for you) or provide a vector in GraphQL.

- **No results from a collection**  
  Likely not inserted. Ensure the corresponding CSV + `*_ids.txt` + `*_labse.npy` exist in `data/outputs/`.

- **Torch / model install issues**  
  The ETL image installs `sentence-transformers` (which pulls PyTorch). On first run it downloads LaBSE.  
  If your environment has no internet, pre-bake a wheel cache or build the image where internet is available.

- **Weaviate not ready / timeout**  
  Check `./bootstrap.sh logs`. Ensure ports `8080` and `50051` are free, and the service can write to `weaviate_data/`.

- **Different host paths**  
  Adjust the `volumes:` mapping in `docker-compose.yml` so the container sees your host `./data/outputs` at `/workspace/data/outputs`.


## üìé Notes

- We keep **DEFAULT_VECTORIZER_MODULE: none** and **inject vectors offline** (LaBSE).  
- You can switch to built-in vectorizers later by enabling modules in Weaviate and adjusting the schema creation.  
- The ETL respects id consistency (stable UUIDs) if your inserter uses UUIDv5 over your row IDs.

---

Happy searching! üôè
